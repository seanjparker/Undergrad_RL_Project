\chapter{Introduction}
\label{cha:intro}
My project aims to implemented and compare some of the reinforcement learning algorithms that have been used to play classic Atari 2600 games. It also compares the results of different tests with these algorithms such as varying the hyperparameters of the network. By observing the effects on the trained agents\footnote{Agent. In this case, agent refers to a trained neural network that takes actions in a chosen environment.} when the hyperparameters are changed we can deduce a set of optimal values such that the networks can play three different Atari 2600 games. Overall, the main features of the project are the following:
\begin{itemize}
  \item Agents with raw pixel data as input, outputting a set of values for the best action.
  \item Agents attempt to find an optimal model of the environment without any prior knowledge.
  \item Visualization of the agents ``brain'' to provide insight into what information the agent is learning.
\end{itemize}

Concerning the first point above, there are two ways in which you can provide inputs to agents that play games. The first (and often simplest) option is by using values from specific locations in RAM\footnote{In this case, RAM refers specifically to the in-game memory.}. If we take the game Pong as an example, we could take information such as the enemy paddle y-position, the ball x,y-position, the players paddle y-position and finally the score. By having this information without any processing required, it helps to reduce the state space.

On the other hand, another option is to take as input the raw pixels from the game, it makes it more difficult to play the game as we need to interpret the pixels to deduce all the information to make optimal moves. This is the approach we take as it more closely simulates how a human player would observe information from the game. Additionally, this project also aims to implement a variety of methods that are described in \ref{dsgn:sec:qlearning} and \ref{dsgn:sec:qlearning:qextra}; the simplest of which is a method called Q-Learning and then further improvements to the Q-Learning called Double Q-Learning and Duelling Q-Learning.

Finally, this project describes methods by which it is possible to visualise the agents ``brain'', there has been much research in recent years into methods to visualise high-dimensional data when investigating neural network and this project explores and implements one such method that will visualise what information the agent has learnt about the environment.

\section{Motivation}
\label{intro:sec:moti}
Over the past 10 years, there has been significant improvement in the RL (reinforcement learning) algorithms. One reason is that computing power has become cheaply available by using discrete graphics cards. For example, for my project, I used an Nvidia GeForce GTX 1070 that provides 1920 CUDA cores that can be used to accelerate training of neural networks. Despite this, RL algorithms are massively computationally expensive and hence take a long time to train.

Over recent years one of the pioneers in this area is DeepMind, which was acquired by Google in 2014, and they developed the DQN (deep q-network) algorithm in 2013 which they demonstrated could learn directly from the raw pixel data of games to achieve either human-level or super-human level performance.

This research was expanded upon by DeepMind and OpenAI which is based on the original DQN by DeepMind. This research focused on trying to approximate a Q-function and thereby infer the optimal policy. On the other hand, there has recently been a focus on other methods such as A3C and PPO which instead seek to directly optimise in the policy space of the environment.

\section{Objectives}
\label{intro:sec:obj}
Further to what was described in section \ref{cha:intro}, there were a few main objectives of the project. Firstly, I chose three games on which I decided to train the agents, Pong, Breakout, and Space Invaders which are shown below in Figure \ref{fig:atari-screenshot}.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.7\textwidth]{chapters/chapter1/images/atari-combined.jpg}
  \caption{Screenshots of Pong, Breakout, Space Invaders (left to right).
    \label{fig:atari-screenshot}
  }
\end{figure}

Secondly, having a method to explore the internals of a trained agent can provide further insight into what the agent is trying to learn. A major benefit for this approach is that a researcher could use this information to determine, for example, where the agent has learnt to focus on the frame based on the filter activation maps. Additionally, it also can point towards poor hyperparameter choice such as a high learning rate when the layers have many dead filters. More discussion of these techniques is provided in Section \ref{bg:sec:cnn-vis}.

\section{Report structure}
\label{intro:sec:report_struc}

This report is divided into three main sections. First, describing the background of the problem and the history of the methods/techniques that underpin reinforcement learning and neural networks. Further, a section providing a detailed discussion of the project design and motivating the choice of some of the decisions made in this project. After that, a section discussing the implementation of the project and system design. Penultimately, we will discuss the evaluation of the project with a comparison of the different techniques and critical analysis. Finally, this report ends with a personal reflection on this project.

\section{Impact of COVID-19}
COVID-19 has caused widespread disruption across the faculty and University as a whole, and unfortunately, it also impacted my plans for the Semester. The circumstances surrounding the pandemic lead to uncertainty around whether it was safe to be at University and made it more difficult to maintain the required level of focus and motivation on the project, mainly as I was concerned about if I should travel home or have the possibility of being quarantined in University accommodation. On the other hand, I know that many people were more affected than I, in comparison my situation was much less disrupted than others.