\chapter{Experiments}
This chapter will experiments that were performed to determine the performance of all the different algorithms, DQN, Double DQN and Duelling DQN. The three different environments are the three Atari games we have discussed before, that is, Pong, Breakout and Space Invaders. Each of the environments provides a different challenge, especially Space Invaders which proposed a unique challenge. This chapter also contains an analysis of the reasons why some of these methods are better performing than others which follows from the discussion in Section \ref{dsgn:sec:qlearning:qextra}.

\section{Atari Network}
This section will describe the choice of hyperparameters during training and additionaly motivate the choices to some of the hyperparameters that were chosen for the evaluation of the algorithms. The network architecture was descirbed completely in Section \ref{imple:cnn} and is also provided in a table in the table \ref{table:network-arch}. This architecture of the agent was used during the training of all the agents for all environments.

\begin{table}[htbp]
  \centering
  \begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
    Layer   & Input    & Filter Size & \# Filters & Stride & Activation function & Output   \\ \hline
    conv\_1 & 4x84x84  & 20x20       & 32         & 4      & ReLU                & 32x20x20 \\
    conv\_2 & 32x20x20 & 9x9         & 64         & 2      & ReLU                & 64x9x9   \\
    conv\_3 & 64x9x9   & 7x7         & 64         & 1      & ReLU                & 64x7x7   \\
    fc\_1   & 64x7x7   & -           & -          & -      & ReLU                & 256      \\
    fc\_2   & 256      & -           & -          & -      & ReLU                & 4        \\ \hline
  \end{tabular}
  \caption{Table of the network architecture
    \label{table:network-arch}
  }
\end{table}

As has been noted previously, the output of the network is sometimes dependent on the game that is currently being trained. The final output varies between 4 and 6 nodes. Addition, the hyperparameters used for all the experiments in the following sections is provided in Appendix (TODO). The only difference was in Space Invaders where the frameskip hyperparameter was set as $k = 3$ instead of the standard $k = 4$ as is used in Pong and Breakout.

\subsection{Pong}
Pong is the first environment which this project was trained using, it is by far the simplest of environments in the Atari 2600 series and provides an excellent testbed for RL algorithms. Due to the simplicity of the game, modern RL methods allows the algorithms to converge to an optimal strategy very quickly, therefore, it is easy to tell if an approach is viable or the implementation contains bugs that affect the performance of the agent.

During the training phase of Pong, it was tested using two methods, that is, DQN and a combination of Duelling-Double DQN. The major reason for combining the two methods (Double and Duelling DQN) is that when used individually, both methods achieve a very similar level of performance, hence, we can produce more interesting results by combining the two methods.

The agent was allowed to play a maximum of 10 million frames of Pong (Pong converges quickly, therefore it doesn't need the 20 million frames that other games like Breakout and Space Invaders require). Additionally, due to the limited compute power available to this project, a single Nvidia GTX 1070, training the agent for 20 million frames takes just over 24 hours. Therefore, the number of opportunities to perform  which results in approximately (TODO) episodes\footnote{An episode is considered complete when the agent loses a life.}. Figure (TODO) shows the results during training of the agent, as can be observed, Pong results in a convergance plot for both the methods described above. In this game, neither method outperforms the other as both algorithms converge to the optimal score of $+21$.

\subsection{Breakout}
Breakout was the second of the three environments uponn which the agent was trained. We used the same network architecture as we did for Pong and the hyperparameters were the same during training with the exception of the number of timesteps for which the agent was trained. In Pong we used 10 million timesteps, however, since Breakout is more complex, it required 20 million timesteps in order to show a trend in the data that the agent is indeed improving over time. It should be noted that in the majority of papers that are referenced in this project it is common to use 100 million timesteps when training on Atari; it would not be feasible to train the agent for this number of timesteps on the hardware used for training as this would take over 5 days for a single algorithm on the environment.

Figure (TODO) shows the results of training the agent of 20 million timesteps and the average reward achieved over every (TODO) timesteps. The (TODO) line shows the results for training the DQN, Double DQN and Double-Duelling DQN in (TODO) respecitvely. We can clearly observe that the DQN algorithm is the worst performing with the duelling-double DQN having the most stable and best performance. We also observe the same results during evaluation of the agent over 1000 episodes as shown in Figure (TODO).

\subsection{Space Invaders}


\section{Visualisation}
In order to evaluate the visualisation of the neural network, we can do this by first, checking that the layers of the network are as we expect. Secondly, we check that the Q-function visualisation plots the shape which we expect it to look like based upon both previous research and the theory of Q-Learning.