One of the main aims of this project was to implement specific algorithms and techniques that are used in deep reinforcement learning to achieve human-like (or super human) performance in Atari 2600 games. In addition, this project describes a method to visualize the Q-function approximator, the convolutional layers and ways to understand the meaning of the networks output predictions. This visualisation can provide useful insights into what information the agent is learning and, importantly, how the agent is learning to act optimally in the environment.

Reinforcement learning is a sub-field in machine learning which which has its roots in Markov decisionn processes and over the past few years has seen rapid and continued devleopment, starting with the introduction of the DQN (Deep Q-Network) algorithm. The main innovation was the way in which the agent processes information, we take raw pixel data as input and produce Q-value predictions, indicating the ``quality'' of a given action. This provided a method for end-to-end learning, without requiring the pre-programming of the environments rules.

This project also investigates improvements to the DQN algorithm such as Double Q-Learning and Duelling Q-Learning which both improve the stability of the algorithm and help prevent a common issue in Q-learning which is the overestimation of Q-values.