\begin{algorithm}[H]
	\SetAlgoNoLine
	\DontPrintSemicolon
	Initialize replay memory $\mathcal{D}$ with maximum capacity $N$\;
	Initialize action-value function $Q$ with random weights $\theta$\;
	Initialize \textbf{target} action-value function $\hat{Q}$ with weights $\theta_{}^{-} = \theta$\;
	\For{timestep = 1, T}{
	Initialise S\;
	\While{S is not done state}{
	With probability $\varepsilon$ choose random action $a_t$\;
	otherwise, pick $a_t = \max_{a}Q(s_t, a;~\theta)$\;
	Take action $a_t$ and observse reward $r_t$ and state $s_{t + 1}$\;
	$Q(s, a) = Q(s_t, a_t) + \alpha(R + \gamma~max_{a}Q(s_{t + 1}, a) - Q(s_t, a_t))$\;
	Store tuple $(s_t, a_t, r_t, s_{t + 1})$ in $\mathcal{D}$\;
	Randomly sample a minibatch of tuples $(s_t, a_t, r_t, s_{t + 1})$ from $\mathcal{D}$\;
	Every $C$ steps copy weights $\theta_{}^{-} = \theta$
	}
	}
	\caption{Deep Q-Learning with Experience replay}
	\label{algo:dqn}
\end{algorithm}